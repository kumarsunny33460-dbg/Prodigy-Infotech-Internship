# 📝 Task-01 – Text Generation with GPT-2
**Prodigy Infotech Internship – Artificial Intelligence**

## 📌 Overview
This project demonstrates the fine-tuning of the GPT-2 language model on a sample dataset to generate human-like text.  
The task was assigned as part of my internship at **Prodigy Infotech** to explore natural language processing (NLP) and transformer-based models.

---

## 🚀 Technologies Used
- **Python**
- **Google Colab** (GPU environment)
- **Hugging Face Transformers**
- **Datasets Library**
- **PyTorch**

---

## 📂 Project Structure



---
## 🔍 Steps Performed
1. **Environment Setup**  
   Installed `transformers`, `datasets`, and `torch` in Google Colab and enabled GPU.

2. **Model & Tokenizer Loading**  
   Used the pre-trained GPT-2 model from Hugging Face.

3. **Dataset Preparation**  
   Created a sample text dataset (can be replaced with custom data).

4. **Tokenization**  
   Converted text into token IDs suitable for GPT-2.

5. **Fine-Tuning**  
   Trained GPT-2 for 3 epochs using the Hugging Face `Trainer`.

6. **Saving & Loading Model**  
   Saved the fine-tuned model for reuse.

7. **Text Generation**  
   Tested model on various prompts.

---

## 📜 Sample Outputs


---

## 📄 Report
A detailed report of the project can be found here:  
📄 [Task-01 GPT-2 Fine-Tuning Report](task_01_gpt2_report.pdf)

---

## 👨‍💻 Author
**Sunny Kumar**  
B.Tech CSE – 2nd Year Student  
Intern at Prodigy Infotech

---


