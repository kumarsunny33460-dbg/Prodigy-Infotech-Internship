# ğŸ“ Task-01 â€“ Text Generation with GPT-2
**Prodigy Infotech Internship â€“ Artificial Intelligence**

## ğŸ“Œ Overview
This project demonstrates the fine-tuning of the GPT-2 language model on a sample dataset to generate human-like text.  
The task was assigned as part of my internship at **Prodigy Infotech** to explore natural language processing (NLP) and transformer-based models.

---

## ğŸš€ Technologies Used
- **Python**
- **Google Colab** (GPU environment)
- **Hugging Face Transformers**
- **Datasets Library**
- **PyTorch**

---

## ğŸ“‚ Project Structure



---
## ğŸ” Steps Performed
1. **Environment Setup**  
   Installed `transformers`, `datasets`, and `torch` in Google Colab and enabled GPU.

2. **Model & Tokenizer Loading**  
   Used the pre-trained GPT-2 model from Hugging Face.

3. **Dataset Preparation**  
   Created a sample text dataset (can be replaced with custom data).

4. **Tokenization**  
   Converted text into token IDs suitable for GPT-2.

5. **Fine-Tuning**  
   Trained GPT-2 for 3 epochs using the Hugging Face `Trainer`.

6. **Saving & Loading Model**  
   Saved the fine-tuned model for reuse.

7. **Text Generation**  
   Tested model on various prompts.

---

## ğŸ“œ Sample Outputs


---

## ğŸ“„ Report
A detailed report of the project can be found here:  
ğŸ“„ [Task-01 GPT-2 Fine-Tuning Report](task_01_gpt2_report.pdf)

---

## ğŸ‘¨â€ğŸ’» Author
**Sunny Kumar**  
B.Tech CSE â€“ 2nd Year Student  
Intern at Prodigy Infotech

---


